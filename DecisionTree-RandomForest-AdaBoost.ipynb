{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a91d7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "Results = {\"Model\":[],\"Accuracy\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ed1e4",
   "metadata": {},
   "source": [
    "## DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f983b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gini(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return 1 - np.sum(ps**2)\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "def misclassification_rate(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return 1 - np.max(ps)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.criterion = criteria[criterion]\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.root = self.grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict(inputs) for inputs in X])\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        # Compute the baseline criterion score\n",
    "        best_score = self.criterion(y)\n",
    "        best_idx, best_thr = None, None\n",
    "\n",
    "        # Loop through all features\n",
    "        for idx in range(self.n_features):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes\n",
    "            num_right = y.tolist().copy()\n",
    "\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "\n",
    "                if i < self.min_samples_leaf or m - i < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                # Ensure no negatives in num_left and num_right\n",
    "                num_left = [max(0, val) for val in num_left]\n",
    "                num_right = [max(0, val) for val in num_right]\n",
    "\n",
    "                score = (i * self.criterion(num_left) + (m - i) * self.criterion(num_right)) / m\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2  # midpoint\n",
    "\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self.best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self.grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self.grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.pred(inputs) for inputs in X])\n",
    "\n",
    "    def pred(self, inputs):\n",
    "        node = self.root\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "\n",
    "    def boost_predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.root\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ba59388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "titanic = titanic.drop(['who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'], axis=1)\n",
    "\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
    "\n",
    "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "titanic['sex'] = label_encoder.fit_transform(titanic['sex'])\n",
    "titanic['embarked'] = label_encoder.fit_transform(titanic['embarked'])\n",
    "titanic['class'] = label_encoder.fit_transform(titanic['class'])\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "\n",
    "train_data, test_data = train_test_split(titanic, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = train_data.drop('survived', axis=1)\n",
    "y_train = train_data['survived']\n",
    "X_test = test_data.drop('survived', axis=1)\n",
    "y_test = test_data['survived']\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "285f1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.78%\n"
     ]
    }
   ],
   "source": [
    "# Create a DecisionTree model\n",
    "criteria = {'gini': gini, 'entropy': entropy, 'misclassification_rate': misclassification_rate}\n",
    "\n",
    "model = DecisionTree(criterion='gini', max_depth=3, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = (predictions == y_test).mean()\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "Results['Model'].append(\"DecisionTreeClassifier\")\n",
    "Results['Accuracy'].append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8de44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa86ec90",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e922be45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, base_estimator, n_estimators=100, max_features='sqrt'):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.estimators_ = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        X_, y_ = resample(X, y, replace=True, n_samples=n_samples)\n",
    "        return X_, y_\n",
    "\n",
    "    def _feature_sampling(self, X, max_features):\n",
    "        n_features = X.shape[1]\n",
    "        if max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        if max_features == 'log2':\n",
    "            max_features = int(np.log2(n_features))\n",
    "        if isinstance(max_features, int):\n",
    "            idxs = np.random.choice(n_features, max_features, replace=False)\n",
    "        return X[:, idxs], idxs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.estimators_ = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            estimator = self.base_estimator\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            X_sample, self.feat_idxs = self._feature_sampling(X_sample, self.max_features)\n",
    "            estimator.fit(X_sample, y_sample)\n",
    "            self.estimators_.append(estimator)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X[:, self.feat_idxs]\n",
    "        pred = np.zeros((X.shape[0], len(self.estimators_)))\n",
    "        for i, estimator in enumerate(self.estimators_):\n",
    "            pred[:, i] = estimator.predict(X)\n",
    "        return np.array([np.bincount(row).argmax() for row in pred.astype(int)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f247007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.22%\n"
     ]
    }
   ],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Drop the 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone' columns\n",
    "titanic = titanic.drop(['who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'], axis=1)\n",
    "\n",
    "# Fill missing 'age' values with the median age\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
    "\n",
    "# Fill missing 'embarked' values with the most common port\n",
    "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])\n",
    "\n",
    "# Convert 'sex', 'class', and 'embarked' to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "titanic['sex'] = label_encoder.fit_transform(titanic['sex'])\n",
    "titanic['embarked'] = label_encoder.fit_transform(titanic['embarked'])\n",
    "titanic['class'] = titanic['class'].cat.codes\n",
    "\n",
    "# Drop rows with missing 'fare'\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_data, test_data = train_test_split(titanic, test_size=0.2, random_state=1)\n",
    "\n",
    "# Separate the 'survived' column from the rest of the data\n",
    "X_train = train_data.drop('survived', axis=1)\n",
    "y_train = train_data['survived']\n",
    "X_test = test_data.drop('survived', axis=1)\n",
    "y_test = test_data['survived']\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "base_estimator = DecisionTree(criterion='gini', max_depth=3, min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "# Create a RandomForest model\n",
    "model = RandomForest(base_estimator=base_estimator, n_estimators=100, max_features='sqrt')\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "\n",
    "Results['Model'].append(\"RandomForestClassifier\")\n",
    "Results['Accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f16518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c80d35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd09a8e7",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "243c3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, base_estimator, n_estimators=50, learning_rate=1.):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        unique_values = np.unique(y)\n",
    "        self.n_classes = len(unique_values)\n",
    "        sample_weight = np.ones(n_samples) / n_samples\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "        self.estimator_errors_ = []\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            sample_weight, estimator, estimator_weight, estimator_error = self._boost(\n",
    "                iboost, X, y, sample_weight\n",
    "            )\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            self.estimators_.append(deepcopy(estimator))\n",
    "            self.estimator_weights_.append(estimator_weight)\n",
    "            self.estimator_errors_.append(estimator_error)\n",
    "\n",
    "            sample_weight /= sample_weight.sum()\n",
    "\n",
    "    def _boost(self, iboost, X, y, sample_weight):\n",
    "        indices = np.random.choice(np.arange(X.shape[0]), size=X.shape[0], p=sample_weight)\n",
    "        X_sample, y_sample = X[indices], y[indices]\n",
    "\n",
    "        estimator = deepcopy(self.base_estimator)\n",
    "        estimator.fit(X_sample, y_sample)\n",
    "\n",
    "        y_predict = estimator.boost_predict(X)\n",
    "        incorrect = y_predict != y\n",
    "        estimator_error = np.mean(np.average(incorrect, weights=sample_weight))\n",
    "\n",
    "        estimator_weight = self.learning_rate * (np.log((1. - estimator_error) / estimator_error) + np.log(self.n_classes - 1))\n",
    "\n",
    "        sample_weight *= np.exp(estimator_weight * incorrect)\n",
    "\n",
    "        return sample_weight, estimator, estimator_weight, estimator_error\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([estimator.boost_predict(X) for estimator in self.estimators_])\n",
    "        avg_pred = np.average(predictions, axis=0, weights=self.estimator_weights_)\n",
    "        return np.sign(avg_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0f936e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 40.78%\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from copy import deepcopy\n",
    "\n",
    "# Your AdaBoost class goes here\n",
    "\n",
    "# Load the dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Drop the 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone' columns\n",
    "titanic = titanic.drop(['who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone'], axis=1)\n",
    "\n",
    "# Fill missing 'age' values with the median age\n",
    "titanic['age'] = titanic['age'].fillna(titanic['age'].median())\n",
    "\n",
    "# Fill missing 'embarked' values with the most common port\n",
    "titanic['embarked'] = titanic['embarked'].fillna(titanic['embarked'].mode()[0])\n",
    "\n",
    "# Convert 'sex', 'embarked', and 'class' to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "titanic['sex'] = label_encoder.fit_transform(titanic['sex'])\n",
    "titanic['class'] = label_encoder.fit_transform(titanic['class'])\n",
    "titanic['embarked'] = label_encoder.fit_transform(titanic['embarked'])\n",
    "\n",
    "# Drop rows with missing 'fare'\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_data, test_data = train_test_split(titanic, test_size=0.2, random_state=1)\n",
    "\n",
    "# Separate the 'survived' column from the rest of the data\n",
    "X_train = train_data.drop('survived', axis=1).values\n",
    "y_train = train_data['survived'].values\n",
    "X_test = test_data.drop('survived', axis=1).values\n",
    "y_test = test_data['survived'].values\n",
    "\n",
    "# Set base estimator as a decision tree\n",
    "base_estimator = DecisionTree(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost model\n",
    "model = AdaBoost(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "Results['Model'].append(\"AdaBoostClassifier\")\n",
    "Results['Accuracy'].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3d031",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abd11e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.597765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.592179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.407821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Accuracy\n",
       "0  DecisionTreeClassifier  0.597765\n",
       "1  RandomForestClassifier  0.592179\n",
       "2      AdaBoostClassifier  0.407821"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results..\n",
    "res = pd.DataFrame(Results)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2183c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
